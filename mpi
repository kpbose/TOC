#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

void prefix_sum(int* local_array, int local_size, MPI_Comm comm) {
    int rank, size, prefix = 0, global_prefix;
    MPI_Comm_rank(comm, &rank);
    MPI_Comm_size(comm, &size);

    for (int i = 0; i < local_size; i++) {
        local_array[i] += prefix;
        prefix = local_array[i];
    }

    if (rank != 0) {
        MPI_Recv(&global_prefix, 1, MPI_INT, rank - 1, 0, comm, MPI_STATUS_IGNORE);
        for (int i = 0; i < local_size; i++) {
            local_array[i] += global_prefix;
        }
    }

    if (rank != size - 1) {
        MPI_Send(&prefix, 1, MPI_INT, rank + 1, 0, comm);
    }
}

int main(int argc, char* argv[]) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int n = 16; // Total number of elements
    int local_size = n / size;

    int* local_array = (int*)malloc(local_size * sizeof(int));

    for (int i = 0; i < local_size; i++) {
        local_array[i] = rank * local_size + i + 1;
    }

    printf("Before Prefix Sum: Rank %d -> ", rank);
    for (int i = 0; i < local_size; i++) {
        printf("%d ", local_array[i]);
    }
    printf("\n");

    prefix_sum(local_array, local_size, MPI_COMM_WORLD);

    printf("After Prefix Sum: Rank %d -> ", rank);
    for (int i = 0; i < local_size; i++) {
        printf("%d ", local_array[i]);
    }
    printf("\n");

    free(local_array);
    MPI_Finalize();
    return 0;
}











#include <stdio.h>
#include <stdlib.h>
#include <omp.h>

void vectorAdd(const float* A, const float* B, float* C, int n) {
    #pragma omp parallel for
    for (int i = 0; i < n; i++) {
        C[i] = A[i] + B[i];
    }
}

int main() {
    int n = 100000; // Size of the vectors
    float* A = (float*)malloc(n * sizeof(float));
    float* B = (float*)malloc(n * sizeof(float));
    float* C = (float*)malloc(n * sizeof(float));

    // Initialize vectors A and B
    for (int i = 0; i < n; i++) {
        A[i] = i;
        B[i] = i * 2;
    }

    // Perform vector addition
    vectorAdd(A, B, C, n);

    // Print a few results for verification
    for (int i = 0; i < 10; i++) {
        printf("C[%d] = %f\n", i, C[i]);
    }

    // Free allocated memory
    free(A);
    free(B);
    free(C);

    return 0;
}












#include <stdio.h>
#include <cuda_runtime.h>

// CUDA Kernel for vector addition
__global__ void vectorAdd(const float* A, const float* B, float* C, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        C[idx] = A[idx] + B[idx];
    }
}

int main() {
    int n = 100000; // Size of the vectors
    size_t size = n * sizeof(float);

    // Host memory allocation
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);
    float* h_C = (float*)malloc(size);

    // Initialize host vectors
    for (int i = 0; i < n; i++) {
        h_A[i] = static_cast<float>(i);
        h_B[i] = static_cast<float>(i * 2);
    }

    // Device memory allocation
    float *d_A, *d_B, *d_C;
    cudaMalloc((void**)&d_A, size);
    cudaMalloc((void**)&d_B, size);
    cudaMalloc((void**)&d_C, size);

    // Copy data from host to device
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Kernel launch configuration
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;

    // Launch the kernel
    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);

    // Copy the result back to the host
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Verify the result
    for (int i = 0; i < 10; i++) {
        printf("C[%d] = %f\n", i, h_C[i]);
    }

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // Free host memory
    free(h_A);
    free(h_B);
    free(h_C);

    return 0;
}
